---
title: "AML project"
author: "Shein Alexander"
date: "14 11 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Loading libraries

```{r}
library(caret)
library(randomForest)
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
set.seed(3332)
```

Reading and cleaning data.

I decided to split train data into train and test and use given test set as validation one to evaluate different models ando choose the best one by out of sample error.

```{r}
trainDataRaw <- read.csv('pml-training.csv')
validationDataRaw <- read.csv('pml-testing.csv')

trainData <- trainDataRaw[, colSums(is.na(trainDataRaw)) == 0]
validationData <- validationDataRaw[, colSums(is.na(trainDataRaw)) == 0]

inTrain <- createDataPartition(y=trainData$classe, p=0.6, list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
# Amount of columns should correspond
dim(validationData)[2]==dim(trainData)[2]
```

Drop columns with near zero variance
```{r}
nsv <- nearZeroVar(trainData)
nsv
```

```{r}
training <- training[,-nsv]
testing <- testing[,-nsv]
validationData <- validationData[,-nsv]
dim(training)
dim(validationData)
```

Removing some more columns
```{r}
unneeded_names <- c("user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp")
training <- training[ , !(names(training) %in% unneeded_names)]
testing <- testing[ , !(names(testing) %in% unneeded_names)]
validationData <- validationData[ , !(names(validationData) %in% unneeded_names)]
dim(validationData)
```
Training random forest and other models to check error

```{r}

modelRF <- train(classe ~ ., data=training, method="rf", trControl=trainControl(method = "cv", number = 2))
print(modelRF)
```
I expect that with such accuracy on train set RF will do well on test one because usually it doesn't over fit.
Especially because model itself is using cross validation to tune the parameters.
```{r}
model2 <- train(classe~., data=training, method="lda", metric = "Accuracy", trControl=trainControl(method = "cv", number = 2))
print(model2)
```
Out of sample error may be bigger here.
```{r}
model3 <- train(classe~., data=training, method="rpart", trControl=trainControl(method = "cv", number = 2))
print(model3)
```

Let's now evaluate random forest 
```{r}
x_test <- testing[ , !(names(testing) %in% c('classe'))]
y_test <- testing[ , (names(testing) %in% c('classe'))]
predictions <- predict(modelRF, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```
So, as expected, it showed very good results.

Evaluating LDA

```{r}
predictions <- predict(model2, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```

Evaluating RPART
```{r}
predictions <- predict(model3, x_test)
# summarize results
confusionMatrix(predictions, y_test)
```


Looks like both LDA and RF worked exceedingly well.
Because their results are quite similar I'll use RF to predict for the validation set.
Now predicting for validation dataset.
```{r}
predictions <- predict(modelRF, validationData, type = 'raw')
```